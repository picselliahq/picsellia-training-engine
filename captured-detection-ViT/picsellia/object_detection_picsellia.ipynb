{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee5-7F5nRwuC"
   },
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HufMiwQCRwuG"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4mgs1JtRwuG",
    "outputId": "8dd58ea5-72ee-480f-e0f4-33be12948008"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from picsellia import Client\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "from picsellia.types.enums import AnnotationFileType, InferenceType\n",
    "from transformers import AutoModelForObjectDetection, TrainingArguments, AutoImageProcessor\n",
    "from transformers import Trainer\n",
    "from transformers import pipeline, TrainerCallback\n",
    "\n",
    "from utils.picsellia import get_experiment, download_data, evaluate_asset, log_metrics\n",
    "from utils.vit import CocoDetection, get_category_mapping, run_evaluation, get_filenames_by_ids, write_metadata_file, \\\n",
    "    read_annotation_file, get_category_mapping, format_coco_annot_to_jsonlines_format, transform_aug_ann, \\\n",
    "    custom_train_test_eval_split, collate_fn, save_annotation_file_images, format_evaluation_results, \\\n",
    "    get_dataset_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMAFbc0PRwuI",
    "outputId": "4ba2b703-ca38-4644-f4f7-f2f4d87ec686"
   },
   "outputs": [],
   "source": [
    "api_token = \"\"\n",
    "client = Client(api_token=api_token, organization_name=\"\")\n",
    "experiment = client.get_experiment_by_id('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, data_dir = download_data(experiment=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations, annotation_file_path = read_annotation_file(dataset=dataset, target_path=data_dir)\n",
    "formatted_coco = format_coco_annot_to_jsonlines_format(annotations=annotations)\n",
    "write_metadata_file(data=formatted_coco, output_path=os.path.join(data_dir,'metadata.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset  = load_dataset(\"imagefolder\", data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_valid_dataset = custom_train_test_eval_split(loaded_dataset=loaded_dataset, test_prop=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_test_valid_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [cat['name'] for cat in annotations['categories']] \n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "labelmap = {str(i): category for i, category in enumerate(categories)}\n",
    "experiment.log(\"labelmap\", labelmap, \"labelmap\", replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz8Yx_PIRwuK"
   },
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7sM-SAGRwuK"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m79hlFARwuL",
    "outputId": "c65aae8e-b4fa-4af2-ac67-f87d15c61599"
   },
   "outputs": [],
   "source": [
    "train_test_valid_dataset[\"train\"] = train_test_valid_dataset[\"train\"].with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in range(len(train_test_valid_dataset['train'])):\n",
    "#     print(sample)\n",
    "#     print(train_test_valid_dataset[\"train\"][sample])\n",
    "\n",
    "## in case there are images with degenerated bowes, remove them \n",
    "# remove_idx = [5325]\n",
    "# keep = [i for i in range(len(train_test_valid_dataset[\"train\"])) if i not in remove_idx]\n",
    "# train_test_valid_dataset[\"train\"] = train_test_valid_dataset[\"train\"].select(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h_4wVG0RwuM"
   },
   "source": [
    "## Training the DETR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NHw4JIgRwuM"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = os.path.join(experiment.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "597sPb2bRwuM"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='constant',\n",
    "    learning_rate=1e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogObjectDetectionMetricsCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            for metric_name, value in logs.items():\n",
    "                log_metrics(metric_name=metric_name, value=value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18R8A3z0RwuM"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_test_valid_dataset[\"train\"],\n",
    "    tokenizer=image_processor,\n",
    "    callbacks=[LogObjectDetectionMetricsCallback]\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=output_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwKte6g4RwuO"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okaWtg_cRwuO"
   },
   "source": [
    "Object detection models are commonly evaluated with a set of <a href=\"https://cocodataset.org/#detection-eval\">COCO-style metrics</a>.\n",
    "You can use one of the existing metrics implementations, but here you'll use the one from `torchvision` to evaluate the final\n",
    "model that you pushed to the Hub.\n",
    "\n",
    "To use the `torchvision` evaluator, you'll need to prepare a ground truth COCO dataset. The API to build a COCO dataset\n",
    "requires the data to be stored in a certain format, so you'll need to save images and annotations to disk first. Just like\n",
    "when you prepared your data for training, the annotations from the `dataset[\"test\"]` need to be formatted. However, images\n",
    "should stay as they are.\n",
    "\n",
    "The evaluation step requires a bit of work, but it can be split in three major steps.\n",
    "First, prepare the `dataset[\"test\"]` set: format the annotations and save the data to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSv8REV2RwuO"
   },
   "source": [
    "Next, prepare an instance of a `CocoDetection` class that can be used with `cocoevaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyX2d1r1RwuO"
   },
   "outputs": [],
   "source": [
    "im_processor = AutoImageProcessor.from_pretrained(output_model_dir)\n",
    "path_output, path_anno = save_annotation_file_images(dataset=train_test_valid_dataset[\"test\"], experiment=experiment, id2label=id2label)\n",
    "test_ds_coco_format = CocoDetection(path_output, im_processor, path_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSwqx8yBRwuO",
    "outputId": "1b27b841-8d22-44cf-fbb3-d4a9b241ed64"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_evaluation(test_ds_coco_format=test_ds_coco_format, im_processor=im_processor, model=model)\n",
    "casted_results = format_evaluation_results(results=results)\n",
    "experiment.log(name='evaluation metrics', type='table', data=casted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLtfz1Y5RwuO"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one image\n",
    "image_path = \"/home/ubuntu/dev/vision-transformers/grape-detector/data/SYH_2017-04-27_1291.jpg\"\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCFoQTZRwuP",
    "outputId": "0dc855d7-4b71-44d9-9336-f8da7c27bb1d"
   },
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(output_model_dir)\n",
    "model = AutoModelForObjectDetection.from_pretrained(output_model_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(output_model_dir)\n",
    "model = AutoModelForObjectDetection.from_pretrained(output_model_dir)\n",
    "dataset_labels = {label.name: label for label in dataset.list_labels()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_image_ids = get_dataset_image_ids(train_test_valid_dataset, \"eval\")\n",
    "id2filename_eval = get_filenames_by_ids(image_ids=eval_image_ids, annotations=annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in list(id2filename_eval.values()):\n",
    "    evaluate_asset(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.compute_evaluations_metrics(inference_type=InferenceType.OBJECT_DETECTION)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
