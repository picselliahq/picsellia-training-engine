{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee5-7F5nRwuC"
   },
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HufMiwQCRwuG"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F4mgs1JtRwuG",
    "outputId": "8dd58ea5-72ee-480f-e0f4-33be12948008",
    "ExecuteTime": {
     "end_time": "2023-08-23T14:01:28.643797692Z",
     "start_time": "2023-08-23T14:01:28.599759801Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from picsellia import Client\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "from picsellia.types.enums import AnnotationFileType, InferenceType\n",
    "from transformers import AutoModelForObjectDetection, TrainingArguments, AutoImageProcessor\n",
    "from transformers import Trainer\n",
    "from transformers import pipeline, TrainerCallback\n",
    "\n",
    "from utils import read_annotation_file, format_coco_annot_to_jsonlines_format, write_metadata_file, custom_train_test_split, transform_aug_ann, collate_fn, save_annotation_file_images, format_evaluation_results, run_evaluation, CocoDetection, get_dataset_image_ids, get_filenames_by_ids, evaluate_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aMAFbc0PRwuI",
    "outputId": "4ba2b703-ca38-4644-f4f7-f2f4d87ec686",
    "ExecuteTime": {
     "end_time": "2023-08-23T13:58:50.568158360Z",
     "start_time": "2023-08-23T13:58:50.231832578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi \u001B[94mhajer\u001B[0m, welcome back. ðŸ¥‘\n",
      "Workspace: \u001B[93myour\u001B[0m organization.\n"
     ]
    }
   ],
   "source": [
    "api_token = \"7ef72b2e908e1a7e931625a2527828e2352807ef\"\n",
    "client = Client(api_token=api_token, organization_name=\"hajer\")\n",
    "experiment = client.get_experiment_by_id('018a1d77-3e0e-77d7-a6c3-7f10942da0bd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:58:55.290830752Z",
     "start_time": "2023-08-23T13:58:55.218205473Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_list = experiment.list_attached_dataset_versions()\n",
    "dataset = dataset_list[0]\n",
    "data_dir = os.path.join(experiment.base_dir, \"data\")\n",
    "dataset.download(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:10.344698858Z",
     "start_time": "2023-08-23T13:59:06.538538894Z"
    }
   },
   "outputs": [],
   "source": [
    "annotations, annotation_file_path = read_annotation_file(dataset=dataset, target_path=data_dir)\n",
    "formatted_coco = format_coco_annot_to_jsonlines_format(annotations=annotations)\n",
    "write_metadata_file(data=formatted_coco, output_path=os.path.join(data_dir,'metadata.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:13.230346791Z",
     "start_time": "2023-08-23T13:59:12.492934032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37b83f20762a4a9387aef4bad0ac1e92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/156 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f00c0f06f812454ab4c18f938c52d672"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4542bb7473db4c36a27b6e0e4b19e9d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "561b562138804d21b5c47a658de5017e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b252faea91654c2abb87f4dd326e48e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_dataset  = load_dataset(\"imagefolder\", data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:25.119286373Z",
     "start_time": "2023-08-23T13:59:25.100432623Z"
    }
   },
   "outputs": [],
   "source": [
    "train_test_valid_dataset = custom_train_test_split(loaded_dataset=loaded_dataset, test_prop=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'width', 'height', 'objects'],\n        num_rows: 131\n    })\n    test: Dataset({\n        features: ['image', 'image_id', 'width', 'height', 'objects'],\n        num_rows: 12\n    })\n    eval: Dataset({\n        features: ['image', 'image_id', 'width', 'height', 'objects'],\n        num_rows: 12\n    })\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_valid_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:25.650346281Z",
     "start_time": "2023-08-23T13:59:25.642530386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:27.068108905Z",
     "start_time": "2023-08-23T13:59:26.851885729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[92mLog labelmap\u001B[0m (id: 018a1d7c-38ff-7684-850d-379d9499de94)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [cat['name'] for cat in annotations['categories']] \n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "labelmap = {str(i): category for i, category in enumerate(categories)}\n",
    "experiment.log(\"labelmap\", labelmap, \"labelmap\", replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz8Yx_PIRwuK"
   },
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A7sM-SAGRwuK",
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:48.898534519Z",
     "start_time": "2023-08-23T13:59:29.219552752Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8m79hlFARwuL",
    "outputId": "c65aae8e-b4fa-4af2-ac67-f87d15c61599",
    "ExecuteTime": {
     "end_time": "2023-08-23T13:59:52.829886914Z",
     "start_time": "2023-08-23T13:59:52.826982011Z"
    }
   },
   "outputs": [],
   "source": [
    "train_test_valid_dataset[\"train\"] = train_test_valid_dataset[\"train\"].with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in range(len(train_test_valid_dataset['train'])):\n",
    "#     print(sample)\n",
    "#     print(train_test_valid_dataset[\"train\"][sample])\n",
    "\n",
    "## in case there are images with degenerated bowes, remove them \n",
    "# remove_idx = [5325]\n",
    "# keep = [i for i in range(len(train_test_valid_dataset[\"train\"])) if i not in remove_idx]\n",
    "# train_test_valid_dataset[\"train\"] = train_test_valid_dataset[\"train\"].select(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h_4wVG0RwuM"
   },
   "source": [
    "## Training the DETR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3NHw4JIgRwuM",
    "ExecuteTime": {
     "end_time": "2023-08-23T14:00:32.464302801Z",
     "start_time": "2023-08-23T13:59:55.485108038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09948c7d94104eefb13c5ef31cfddae2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([12, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T14:00:38.062278019Z",
     "start_time": "2023-08-23T14:00:38.058084577Z"
    }
   },
   "outputs": [],
   "source": [
    "output_model_dir = os.path.join(experiment.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "597sPb2bRwuM"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type='constant',\n",
    "    learning_rate=1e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogObjectDetectionMetricsCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "\n",
    "        if state.is_local_process_zero:\n",
    "            for metric_name, value in logs.items():\n",
    "                if value < 1000:\n",
    "\n",
    "                    if metric_name in ['train_loss', 'total_flos', 'train_steps_per_second', 'train_samples_per_second',\n",
    "                                       'train_runtime']:\n",
    "                        experiment.log(str(metric_name), float(value), 'value')\n",
    "                    else:\n",
    "                        experiment.log(str(metric_name), float(value), 'line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "18R8A3z0RwuM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/vision-transformers/.venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 08:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.117900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=420, training_loss=3.4973063241867792, metrics={'train_runtime': 520.0942, 'train_samples_per_second': 6.23, 'train_steps_per_second': 0.808, 'total_flos': 1.5484674023424e+18, 'train_loss': 3.4973063241867792, 'epoch': 30.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_test_valid_dataset[\"train\"],\n",
    "    tokenizer=image_processor,\n",
    "    callbacks=[LogObjectDetectionMetricsCallback]\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=output_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwKte6g4RwuO"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okaWtg_cRwuO"
   },
   "source": [
    "Object detection models are commonly evaluated with a set of <a href=\"https://cocodataset.org/#detection-eval\">COCO-style metrics</a>.\n",
    "You can use one of the existing metrics implementations, but here you'll use the one from `torchvision` to evaluate the final\n",
    "model that you pushed to the Hub.\n",
    "\n",
    "To use the `torchvision` evaluator, you'll need to prepare a ground truth COCO dataset. The API to build a COCO dataset\n",
    "requires the data to be stored in a certain format, so you'll need to save images and annotations to disk first. Just like\n",
    "when you prepared your data for training, the annotations from the `dataset[\"test\"]` need to be formatted. However, images\n",
    "should stay as they are.\n",
    "\n",
    "The evaluation step requires a bit of work, but it can be split in three major steps.\n",
    "First, prepare the `dataset[\"test\"]` set: format the annotations and save the data to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSv8REV2RwuO"
   },
   "source": [
    "Next, prepare an instance of a `CocoDetection` class that can be used with `cocoevaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "DyX2d1r1RwuO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "im_processor = AutoImageProcessor.from_pretrained(output_model_dir)\n",
    "path_output, path_anno = save_annotation_file_images(dataset=train_test_valid_dataset[\"test\"], experiment=experiment, id2label=id2label)\n",
    "test_ds_coco_format = CocoDetection(path_output, im_processor, path_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "oSwqx8yBRwuO",
    "outputId": "1b27b841-8d22-44cf-fbb3-d4a9b241ed64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:31<00:00, 10.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.079\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.019\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.051\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.161\n",
      "{'iou_bbox': {'AP-IoU=0.50:0.95-area=all-maxDets=100': 0.012445183122175836, 'AP-IoU=0.50-area=all-maxDets=100': 0.035726408052333576, 'AP-IoU=0.75-area=all-maxDets=100': 0.007142415373835251, 'AP-IoU=0.50:0.95-area=small-maxDets=100': 0.005269675319251388, 'AP-IoU=0.50:0.95-area=medium-maxDets=100': 0.0207959731645528, 'AP-IoU=0.50:0.95-area=large-maxDets=100': 0.07910861863813597, 'AR-IoU=0.50:0.95-area=all-maxDets=1': 0.004692312821993718, 'AR-IoU=0.50:0.95-area=all-maxDets=10': 0.01904160489808846, 'AR-IoU=0.50:0.95-area=all-maxDets=100': 0.04086775125540654, 'AR-IoU=0.50:0.95-area=small-maxDets=100': 0.01977514455081171, 'AR-IoU=0.50:0.95-area=medium-maxDets=100': 0.051431341425814486, 'AR-IoU=0.50:0.95-area=large-maxDets=100': 0.16052469135802466}}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[92mLog evaluation metrics\u001B[0m (id: 018a21a3-74f6-7e27-aa46-f2df84f60aa5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = run_evaluation(test_ds_coco_format=test_ds_coco_format, im_processor=im_processor, model=model)\n",
    "casted_results = format_evaluation_results(results=results)\n",
    "experiment.log(name='evaluation metrics', type='table', data=casted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLtfz1Y5RwuO"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one image\n",
    "image_path = \"/home/ubuntu/dev/vision-transformers/grape-detector/data/SYH_2017-04-27_1291.jpg\"\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8jCFoQTZRwuP",
    "outputId": "0dc855d7-4b71-44d9-9336-f8da7c27bb1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected grape with confidence 0.887 at location [174.96, 798.94, 337.52, 1209.99]\n",
      "Detected grape with confidence 0.777 at location [407.09, 592.26, 531.31, 750.52]\n",
      "Detected grape with confidence 0.946 at location [170.18, 798.43, 315.74, 1022.5]\n",
      "Detected grape with confidence 0.608 at location [1563.34, 825.6, 1672.37, 926.45]\n",
      "Detected grape with confidence 0.504 at location [892.52, 732.31, 997.44, 945.75]\n",
      "Detected grape with confidence 0.619 at location [206.25, 389.26, 370.77, 628.61]\n",
      "Detected grape with confidence 0.949 at location [1373.77, 612.05, 1521.82, 1013.14]\n",
      "Detected grape with confidence 0.603 at location [1376.86, 597.86, 1513.69, 907.13]\n",
      "Detected grape with confidence 0.896 at location [1252.14, 508.41, 1406.99, 708.07]\n",
      "Detected grape with confidence 0.922 at location [403.47, 594.29, 550.72, 964.19]\n",
      "Detected grape with confidence 0.604 at location [499.73, 709.88, 637.18, 902.48]\n",
      "Detected grape with confidence 0.775 at location [216.86, 1023.18, 337.33, 1208.56]\n",
      "Detected grape with confidence 0.854 at location [218.13, 389.7, 366.83, 590.48]\n",
      "Detected grape with confidence 0.569 at location [403.44, 632.57, 549.86, 974.89]\n",
      "Detected grape with confidence 0.958 at location [496.4, 712.0, 643.92, 918.26]\n",
      "Detected grape with confidence 0.531 at location [197.23, 384.98, 370.94, 663.74]\n",
      "Detected grape with confidence 0.656 at location [169.57, 590.92, 369.71, 766.69]\n",
      "Detected grape with confidence 0.611 at location [404.84, 605.18, 533.58, 791.35]\n",
      "Detected grape with confidence 0.653 at location [403.66, 609.12, 543.71, 983.91]\n",
      "Detected grape with confidence 0.688 at location [745.84, 107.02, 911.55, 279.54]\n",
      "Detected grape with confidence 0.73 at location [430.22, 818.95, 534.41, 975.23]\n",
      "Detected grape with confidence 0.664 at location [174.7, 588.67, 370.13, 887.59]\n",
      "Detected grape with confidence 0.579 at location [1250.3, 514.23, 1403.7, 721.83]\n",
      "Detected grape with confidence 0.849 at location [192.22, 390.41, 374.43, 746.19]\n",
      "Detected grape with confidence 0.555 at location [192.3, 820.32, 309.47, 1030.07]\n",
      "Detected grape with confidence 0.933 at location [1830.55, 825.93, 2013.54, 1023.02]\n",
      "Detected grape with confidence 0.589 at location [1863.9, 840.09, 1988.61, 1006.85]\n",
      "Detected grape with confidence 0.682 at location [1249.13, 497.77, 1408.41, 815.32]\n",
      "Detected grape with confidence 0.948 at location [884.82, 712.04, 1008.27, 955.66]\n",
      "Detected grape with confidence 0.634 at location [1368.22, 591.17, 1521.14, 1019.52]\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(output_model_dir)\n",
    "model = AutoModelForObjectDetection.from_pretrained(output_model_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(output_model_dir)\n",
    "model = AutoModelForObjectDetection.from_pretrained(output_model_dir)\n",
    "dataset_labels = {label.name: label for label in dataset.list_labels()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_image_ids = get_dataset_image_ids(train_test_valid_dataset, \"eval\")\n",
    "id2filename_eval = get_filenames_by_ids(image_ids=eval_image_ids, annotations=annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in list(id2filename_eval.values()):\n",
    "    evaluate_asset(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JobV1 018a21ab-e898-7f6c-bb34-dafa9f10f195: JobStatus.RUNNING"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.compute_evaluations_metrics(inference_type=InferenceType.OBJECT_DETECTION)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
